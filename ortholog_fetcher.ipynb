{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yongchanzzz/bioinformatics/blob/main/ortholog_fetcher.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üß¨ Ortholog Fetcher (Colab-ready, MMseqs2 ‚Üí UniProt ‚Üí BLAST)\n",
        "\n",
        "Pipeline order:\n",
        "1. MMseqs2 local search (proteome DB per species)\n",
        "2. UniProt API gene search\n",
        "3. BLAST (EBI REST API, stype=protein)\n",
        "\n",
        "Output: CSV with columns species_name,taxid,chain_A,chain_B,...\n"
      ],
      "metadata": {
        "id": "XZMXvwEjIPxr"
      },
      "id": "XZMXvwEjIPxr"
    },
    {
      "cell_type": "code",
      "source": [
        "#@title üîß Install MMseqs2\n",
        "!wget -q https://mmseqs.com/latest/mmseqs-linux-avx2.tar.gz -O mmseqs2.tar.gz\n",
        "!tar xfz mmseqs2.tar.gz\n",
        "!sudo mv mmseqs/bin/mmseqs /usr/local/bin/\n",
        "\n",
        "import subprocess, sys\n",
        "try:\n",
        "    out = subprocess.run([\"mmseqs\",\"-h\"], capture_output=True, text=True, check=False)\n",
        "    banner = out.stdout.splitlines()\n",
        "    print(\"\\n\".join(banner[:5]))  # print just the top lines with version\n",
        "except Exception as e:\n",
        "    print(\"MMseqs2 installed, but couldn't read banner:\", e)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "3I-1FsU9IUps"
      },
      "id": "3I-1FsU9IUps",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title üîß Install Foldseek\n",
        "!wget -q https://mmseqs.com/foldseek/foldseek-linux-avx2.tar.gz -O foldseek.tar.gz\n",
        "!tar xfz foldseek.tar.gz\n",
        "!sudo mv foldseek/bin/foldseek /usr/local/bin/\n",
        "\n",
        "import subprocess\n",
        "out = subprocess.run([\"foldseek\",\"-h\"], capture_output=True, text=True)\n",
        "print(\"\\n\".join(out.stdout.splitlines()[:8]))\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "pir0oC-Oyr2c"
      },
      "id": "pir0oC-Oyr2c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ‚öôÔ∏è Parameters\n",
        "# Seeds: UniProt accessions (comma-separated)\n",
        "SEED_IDS = \"P25705,P06576,P36542,P30049,P56381\"  #@param {type:\"string\"}\n",
        "\n",
        "# Column aliases (same length as seeds; blanks auto-filled: chain_A, chain_B, ‚Ä¶)\n",
        "CHAIN_NAMES = \"ATP5F1A,ATP5F1B,ATP5F1G,ATP5F1D,ATP5F1E\"  #@param {type:\"string\"}\n",
        "\n",
        "# Species: scientific names (optionally 'Name (taxid)')\n",
        "SPECIES_INPUT_NAMES = \"Mus musculus, Danio rerio, Ciona intestinalis, Drosophila melanogaster, Caenorhabditis elegans\"  #@param {type:\"string\"}\n",
        "SPECIES_INPUT_TAXIDS = \"\"  #@param {type:\"string\"}  # e.g. \"10090,7955\" to force\n",
        "\n",
        "# Optional interactive check before building DBs\n",
        "INTERACTIVE_REVIEW = True  #@param {type:\"boolean\"}\n",
        "\n",
        "# Proteome preference\n",
        "USE_REVIEWED_ONLY = False   #@param {type:\"boolean\"}\n",
        "\n",
        "# MMseqs2 (sequence homology)\n",
        "USE_MMSEQS2 = True          #@param {type:\"boolean\"}\n",
        "THREADS = 2                 #@param {type:\"integer\"}\n",
        "MMSEQS2_SENS = 7.5          #@param {type:\"number\"}\n",
        "MMSEQS2_MAX_SEQS = 200      #@param {type:\"integer\"}\n",
        "MMSEQS2_MAX_EVALUE = 1e-3    #@param {type:\"number\"}\n",
        "MMSEQS2_MIN_BITS   = 50.0    #@param {type:\"number\"}\n",
        "MMSEQS2_MIN_PIDENT = 5.0   #@param {type:\"number\"}    # %\n",
        "MMSEQS2_MIN_QCOV   = 0.30   #@param {type:\"number\"}    # 0..1\n",
        "MMSEQS2_MIN_TCOV   = 0.50   #@param {type:\"number\"}    # 0..1\n",
        "\n",
        "# Foldseek (structural; last resort)\n",
        "USE_FOLDSEEK = False        #@param {type:\"boolean\"}\n",
        "USE_BLAST = False        #@param {type:\"boolean\"}\n",
        "FOLDSEEK_DB = \"\"            #@param {type:\"string\"}    # local DB path if available\n",
        "FOLDSEEK_SENS = 9.5         #@param {type:\"number\"}\n",
        "FOLDSEEK_MAX_SEQS = 1000    #@param {type:\"integer\"}\n",
        "FOLDSEEK_ALIGNMENT_TYPE = 2 #@param {type:\"integer\"}   # 2 local, 1 global\n",
        "FOLDSEEK_MAX_EVALUE = 1e-2  #@param {type:\"number\"}\n",
        "FOLDSEEK_MIN_ALNTMS = 0.45  #@param {type:\"number\"}\n",
        "FOLDSEEK_MIN_LDDT   = 0.00  #@param {type:\"number\"}\n",
        "FOLDSEEK_MIN_QCOV   = 0.30  #@param {type:\"number\"}\n",
        "FOLDSEEK_MIN_TCOV   = 0.30  #@param {type:\"number\"}\n",
        "FOLDSEEK_COV_MODE   = 0     #@param {type:\"integer\"}\n",
        "\n",
        "OUTPUT_BASENAME = \"ortholog_fetcher\"\n",
        "VERBOSE = True              #@param {type:\"boolean\"}\n",
        "\n",
        "import re, os, csv, math, time, json, tempfile, subprocess, unicodedata, requests\n",
        "import pandas as pd\n",
        "\n",
        "def _excel_col(n: int) -> str:\n",
        "    s = \"\"\n",
        "    while True:\n",
        "        n, r = divmod(n, 26)\n",
        "        s = chr(65 + r) + s\n",
        "        if n == 0: break\n",
        "        n -= 1\n",
        "    return s\n",
        "\n",
        "SEED_LIST = [s.strip() for s in SEED_IDS.split(\",\") if s.strip()]\n",
        "_raw_names = [c.strip() for c in CHAIN_NAMES.split(\",\")] if CHAIN_NAMES else []\n",
        "CHAIN_ALIASES = []\n",
        "for i in range(len(SEED_LIST)):\n",
        "    label = _raw_names[i] if i < len(_raw_names) and _raw_names[i] else f\"chain_{_excel_col(i)}\"\n",
        "    CHAIN_ALIASES.append(label)\n",
        "\n",
        "print(\"Seeds:\", SEED_LIST)\n",
        "print(\"Aliases:\", CHAIN_ALIASES)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "bdeZ71kd2bAJ"
      },
      "id": "bdeZ71kd2bAJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title üß≠ Species resolver (NCBI ‚Üí UniProt; adds seed-origin species)\n",
        "import re, time, json, requests\n",
        "from IPython.display import display\n",
        "try:\n",
        "    import ipywidgets as widgets\n",
        "except Exception:\n",
        "    widgets = None\n",
        "\n",
        "NCBI_TOOL = \"OrthologFetcherColab\"\n",
        "NCBI_EMAIL = \"your.email@example.com\"\n",
        "UNI_BASE = \"https://rest.uniprot.org\"\n",
        "UNI_UNIPROTKB = f\"{UNI_BASE}/uniprotkb\"\n",
        "UNI_TAXONOMY = f\"{UNI_BASE}/taxonomy\"\n",
        "NCBI_EUTILS  = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils\"\n",
        "\n",
        "def _parse_name_taxid_token(tok: str):\n",
        "    tok = tok.strip()\n",
        "    m = re.match(r\"^(.*)\\((\\d+)\\)\\s*$\", tok)\n",
        "    if m:\n",
        "        return m.group(1).strip(), int(m.group(2))\n",
        "    return tok, None\n",
        "\n",
        "def _ncbi_taxid_from_name(name: str) -> int | None:\n",
        "    params = {\n",
        "        \"db\":\"taxonomy\",\n",
        "        \"term\": f'{name}[SCIN] OR {name}[CN]',\n",
        "        \"retmode\":\"json\",\n",
        "        \"tool\": NCBI_TOOL,\n",
        "        \"email\": NCBI_EMAIL,\n",
        "    }\n",
        "    r = requests.get(f\"{NCBI_EUTILS}/esearch.fcgi\", params=params, timeout=30)\n",
        "    if r.status_code != 200:\n",
        "        return None\n",
        "    ids = r.json().get(\"esearchresult\", {}).get(\"idlist\", [])\n",
        "    if not ids:\n",
        "        return None\n",
        "    taxid = ids[0]\n",
        "    time.sleep(0.34)  # be polite to NCBI\n",
        "    return int(taxid)\n",
        "\n",
        "def _uniprot_taxid_from_name(name: str) -> int | None:\n",
        "    r = requests.get(f\"{UNI_TAXONOMY}/search\",\n",
        "                     params={\"query\":name,\"format\":\"json\",\n",
        "                             \"fields\":\"scientificName,commonName,taxonId,rank\",\"size\":25},\n",
        "                     timeout=30)\n",
        "    if r.status_code != 200:\n",
        "        return None\n",
        "    res = r.json().get(\"results\", [])\n",
        "    if not res:\n",
        "        return None\n",
        "    for it in res:\n",
        "        if it.get(\"scientificName\",\"\").lower()==name.lower() and (it.get(\"rank\",\"\") or \"\").lower()==\"species\":\n",
        "            return int(it[\"taxonId\"])\n",
        "    for it in res:\n",
        "        if it.get(\"scientificName\",\"\").lower()==name.lower():\n",
        "            return int(it[\"taxonId\"])\n",
        "    for it in res:\n",
        "        if (it.get(\"rank\",\"\") or \"\").lower()==\"species\":\n",
        "            return int(it[\"taxonId\"])\n",
        "    return int(res[0][\"taxonId\"])\n",
        "\n",
        "def resolve_species_inputs(names_csv: str, taxids_csv: str):\n",
        "    out = []\n",
        "    if taxids_csv.strip():\n",
        "        for tok in [t for t in taxids_csv.split(\",\") if t.strip()]:\n",
        "            tid = int(tok.strip())\n",
        "            sci = f\"taxid:{tid}\"\n",
        "            try:\n",
        "                rr = requests.get(f\"{UNI_TAXONOMY}/{tid}\", params={\"format\":\"json\"}, timeout=30)\n",
        "                if rr.status_code==200:\n",
        "                    sci = rr.json().get(\"scientificName\", sci)\n",
        "            except Exception:\n",
        "                pass\n",
        "            out.append((sci, tid))\n",
        "        return out\n",
        "\n",
        "    for name_tok in [t for t in names_csv.split(\",\") if t.strip()]:\n",
        "        nm, tid = _parse_name_taxid_token(name_tok)\n",
        "        if tid:\n",
        "            out.append((nm, tid)); continue\n",
        "        tid = _ncbi_taxid_from_name(nm) or _uniprot_taxid_from_name(nm)\n",
        "        if tid is None:\n",
        "            print(f\"[WARN] Could not resolve taxid for '{nm}'. \"\n",
        "                  f\"Consider entering as 'Name (taxid)' or via SPECIES_INPUT_TAXIDS.\")\n",
        "        out.append((nm, tid))\n",
        "    return out\n",
        "\n",
        "def up_get_entry(acc):\n",
        "    r = requests.get(f\"{UNI_UNIPROTKB}/{acc}\", params={\"format\":\"json\"}, timeout=30)\n",
        "    if r.status_code==200: return r.json()\n",
        "    return None\n",
        "\n",
        "def up_taxid_of_acc(acc):\n",
        "    js = up_get_entry(acc)\n",
        "    return js.get(\"organism\",{}).get(\"taxonId\") if js else None\n",
        "\n",
        "def species_name_from_taxid(taxid: int) -> str:\n",
        "    try:\n",
        "        rr = requests.get(f\"{UNI_TAXONOMY}/{taxid}\", params={\"format\":\"json\"}, timeout=30)\n",
        "        if rr.status_code==200:\n",
        "            return rr.json().get(\"scientificName\", f\"taxid:{taxid}\")\n",
        "    except Exception:\n",
        "        pass\n",
        "    return f\"taxid:{taxid}\"\n",
        "\n",
        "# 1) Resolve the user-provided list\n",
        "SPECIES_RESOLVED = resolve_species_inputs(SPECIES_INPUT_NAMES, SPECIES_INPUT_TAXIDS)\n",
        "print(\"Resolved species (pre-review):\", SPECIES_RESOLVED)\n",
        "\n",
        "# 2) Add seed-origin species (seeds can be from several organisms)\n",
        "SEED_LIST = [s.strip() for s in SEED_IDS.split(\",\") if s.strip()]\n",
        "SEED_ORIGINS = {acc: up_taxid_of_acc(acc) for acc in SEED_LIST}\n",
        "SEED_ORIGIN_SPECIES = {\n",
        "    acc: (species_name_from_taxid(tid) if tid else None)\n",
        "    for acc, tid in SEED_ORIGINS.items()\n",
        "}\n",
        "\n",
        "# 3) Build union set: user species + seed-origin species\n",
        "_species_by_taxid = {}\n",
        "for nm, tid in SPECIES_RESOLVED:\n",
        "    if tid: _species_by_taxid[tid] = nm\n",
        "\n",
        "for acc, tid in SEED_ORIGINS.items():\n",
        "    if tid and tid not in _species_by_taxid:\n",
        "        _species_by_taxid[tid] = SEED_ORIGIN_SPECIES.get(acc) or species_name_from_taxid(tid)\n",
        "\n",
        "ALL_SPECIES = [(name, tid) for tid, name in sorted(_species_by_taxid.items())]\n",
        "\n",
        "print(\"Seed origin taxids:\", SEED_ORIGINS)\n",
        "print(\"Augmented species set:\", ALL_SPECIES)\n",
        "\n",
        "# Optional interactive review\n",
        "if INTERACTIVE_REVIEW and widgets is not None:\n",
        "    name_boxes = []\n",
        "    taxid_boxes = []\n",
        "    rows = []\n",
        "    for nm, tid in ALL_SPECIES:\n",
        "        nb = widgets.Text(value=str(nm), layout=widgets.Layout(width=\"45%\"))\n",
        "        tb = widgets.Text(value=\"\" if tid is None else str(tid), layout=widgets.Layout(width=\"20%\"))\n",
        "        name_boxes.append(nb); taxid_boxes.append(tb)\n",
        "        rows.append(widgets.HBox([nb, tb]))\n",
        "    apply_btn = widgets.Button(description=\"Use these\", button_style=\"success\")\n",
        "    out_lbl = widgets.HTML()\n",
        "\n",
        "    def _apply(_):\n",
        "        global ALL_SPECIES\n",
        "        new_list=[]\n",
        "        for nb, tb in zip(name_boxes, taxid_boxes):\n",
        "            nm = nb.value.strip()\n",
        "            t  = tb.value.strip()\n",
        "            if not nm: continue\n",
        "            tax = int(t) if t.isdigit() else None\n",
        "            new_list.append((nm, tax))\n",
        "        # Deduplicate by taxid (keep last name entered)\n",
        "        dedup = {}\n",
        "        for nm, tid in new_list:\n",
        "            if tid: dedup[tid] = nm\n",
        "        ALL_SPECIES = [(nm, tid) for tid, nm in dedup.items()]\n",
        "        out_lbl.value = f\"<b>Updated species:</b> {ALL_SPECIES}\"\n",
        "\n",
        "    apply_btn.on_click(_apply)\n",
        "    display(widgets.HTML(\"<b>Review/override species and taxids:</b>\"))\n",
        "    for r in rows: display(r)\n",
        "    display(apply_btn, out_lbl)\n",
        "else:\n",
        "    print(\"Interactive review disabled or ipywidgets unavailable.\")\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "4lijnwVI2dVH"
      },
      "id": "4lijnwVI2dVH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title üß± Download proteomes & build MMseqs2 DBs\n",
        "import os, io, gzip, time, subprocess, requests\n",
        "\n",
        "os.makedirs(\"dbs\", exist_ok=True)\n",
        "os.makedirs(\"mmseqs_tmp\", exist_ok=True)\n",
        "\n",
        "UNI_STREAM = \"https://rest.uniprot.org/uniprotkb/stream\"\n",
        "_HEADERS = {\"User-Agent\": \"OrthologFetcher/1.0\", \"Accept\": \"*/*\"}\n",
        "\n",
        "def _sh(cmd, check=True):\n",
        "    p = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
        "    if check and p.returncode != 0:\n",
        "        print(p.stdout)\n",
        "        raise subprocess.CalledProcessError(p.returncode, cmd)\n",
        "    return p.stdout\n",
        "\n",
        "def _count_headers(fa):\n",
        "    if not os.path.exists(fa):\n",
        "        return 0\n",
        "    n = 0\n",
        "    with open(fa, 'r', errors='ignore') as fh:\n",
        "        for ln in fh:\n",
        "            if ln.startswith(\">\"):\n",
        "                n += 1\n",
        "    return n\n",
        "\n",
        "def _download_uniprot_fasta(taxid: int, out_fa: str, reviewed_only: bool, retries: int = 4):\n",
        "    \"\"\"\n",
        "    Download proteome FASTA via UniProt stream API.\n",
        "    Handles URL-encoding and gzip transparently.\n",
        "    \"\"\"\n",
        "    query = f\"(organism_id:{taxid})\"\n",
        "    if reviewed_only:\n",
        "        query += \" AND (reviewed:true)\"  # contains a space ‚Üí MUST be URL-encoded\n",
        "\n",
        "    params = {\n",
        "        \"format\": \"fasta\",\n",
        "        \"compressed\": \"true\",   # server returns gzipped stream\n",
        "        \"query\": query,\n",
        "    }\n",
        "\n",
        "    # Retry politely with backoff\n",
        "    last_err = None\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            print(f\"  [DL] {taxid} {'Swiss-Prot' if reviewed_only else 'Swiss+TrEMBL'}\")\n",
        "            r = requests.get(UNI_STREAM, params=params, headers=_HEADERS, timeout=120, stream=True)\n",
        "            r.raise_for_status()\n",
        "            # Write the gzipped response to memory/file, then decompress\n",
        "            gz_path = out_fa + \".gz\"\n",
        "            with open(gz_path, \"wb\") as fh:\n",
        "                for chunk in r.iter_content(chunk_size=1 << 20):\n",
        "                    if chunk:\n",
        "                        fh.write(chunk)\n",
        "            # Decompress\n",
        "            with gzip.open(gz_path, \"rb\") as gzh, open(out_fa, \"wb\") as out:\n",
        "                out.write(gzh.read())\n",
        "            os.remove(gz_path)\n",
        "\n",
        "            n = _count_headers(out_fa)\n",
        "            print(f\"  [OK] FASTA headers: {n}\")\n",
        "            if n == 0:\n",
        "                # Empty file: treat as failure so caller can fallback\n",
        "                try: os.remove(out_fa)\n",
        "                except: pass\n",
        "                raise RuntimeError(\"Downloaded FASTA is empty\")\n",
        "            return\n",
        "        except Exception as e:\n",
        "            last_err = e\n",
        "            if attempt < retries - 1:\n",
        "                sleep_s = min(2 ** attempt, 10)\n",
        "                print(f\"  [WARN] download failed (attempt {attempt+1}/{retries}): {e}. Retrying in {sleep_s}s‚Ä¶\")\n",
        "                time.sleep(sleep_s)\n",
        "            else:\n",
        "                print(f\"  [ERR] download failed: {e}\")\n",
        "                raise\n",
        "\n",
        "def _ensure_mmseqs_db(taxid: int):\n",
        "    fa = f\"dbs/{taxid}.fasta\"\n",
        "    if not os.path.exists(fa) or os.path.getsize(fa) == 0:\n",
        "        raise RuntimeError(f\"FASTA missing/empty: {fa}\")\n",
        "    dbdir = f\"dbs/{taxid}\"\n",
        "    # detect if DB looks already built+indexed\n",
        "    has_db = os.path.exists(dbdir) or os.path.exists(dbdir + \".dbtype\")\n",
        "    has_idx = os.path.exists(dbdir + \".index\") or os.path.exists(os.path.join(dbdir, \"index\"))\n",
        "    if has_db and has_idx:\n",
        "        print(f\"  [=] DB OK for {taxid}\")\n",
        "        return\n",
        "    if not has_db:\n",
        "        print(f\"  [DB] Creating DB for {taxid}\")\n",
        "        _sh([\"mmseqs\", \"createdb\", fa, dbdir])\n",
        "    print(f\"  [IX] Indexing\")\n",
        "    # Some environments return non-zero if index already exists ‚Üí don't hard-fail\n",
        "    _sh([\"mmseqs\", \"createindex\", dbdir, \"mmseqs_tmp\", \"--threads\", str(THREADS)], check=False)\n",
        "\n",
        "for sp, tid in ALL_SPECIES:\n",
        "    if not tid:\n",
        "        print(f\"[SKIP] {sp}: no taxid\")\n",
        "        continue\n",
        "    print(f\"\\n[{sp}] taxid={tid}\")\n",
        "    fa = f\"dbs/{tid}.fasta\"\n",
        "    need_dl = not os.path.exists(fa) or _count_headers(fa) == 0\n",
        "    if need_dl:\n",
        "        try:\n",
        "            _download_uniprot_fasta(tid, fa, reviewed_only=USE_REVIEWED_ONLY)\n",
        "        except Exception:\n",
        "            if USE_REVIEWED_ONLY:\n",
        "                print(\"  [WARN] Swiss-Prot empty ‚Üí fallback to full UniProt\")\n",
        "                _download_uniprot_fasta(tid, fa, reviewed_only=False)\n",
        "            else:\n",
        "                raise\n",
        "    _ensure_mmseqs_db(tid)\n",
        "\n",
        "print(\"\\n[Build] done.\")\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "wvgtWF-a2gpL"
      },
      "id": "wvgtWF-a2gpL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title üîé UniProt helpers & methods\n",
        "UNI_BASE = \"https://rest.uniprot.org\"\n",
        "UNI_KB   = f\"{UNI_BASE}/uniprotkb\"\n",
        "\n",
        "def _get_json(url, params=None, timeout=45):\n",
        "    for attempt in range(4):\n",
        "        try:\n",
        "            r = requests.get(url, params=params, timeout=timeout)\n",
        "            r.raise_for_status()\n",
        "            return r.json()\n",
        "        except requests.RequestException:\n",
        "            if attempt == 3: raise\n",
        "            time.sleep(2**attempt)\n",
        "\n",
        "def get_uniprot_entry_json(acc: str):\n",
        "    return _get_json(f\"{UNI_KB}/{acc}\", params={\"format\":\"json\"})\n",
        "\n",
        "def get_uniprot_sequence(acc: str):\n",
        "    r = requests.get(f\"{UNI_KB}/{acc}.fasta\", timeout=45)\n",
        "    if r.status_code != 200: return None\n",
        "    seq = []\n",
        "    for ln in r.text.splitlines():\n",
        "        if ln.startswith(\">\"): continue\n",
        "        seq.append(ln.strip())\n",
        "    return \"\".join(seq) if seq else None\n",
        "\n",
        "def _seed_gene_and_names(seed_js: dict):\n",
        "    gene = None\n",
        "    try:\n",
        "        g0 = seed_js.get(\"genes\", [])\n",
        "        if g0 and \"geneName\" in g0[0]:\n",
        "            gene = g0[0][\"geneName\"].get(\"value\")\n",
        "    except Exception:\n",
        "        pass\n",
        "    pref = None\n",
        "    try:\n",
        "        pref = seed_js[\"proteinDescription\"][\"recommendedName\"][\"fullName\"][\"value\"]\n",
        "    except Exception:\n",
        "        pass\n",
        "    return gene, pref\n",
        "\n",
        "def method_uniprot_quick_name(seed_acc: str, taxid: int, prefer_reviewed=True):\n",
        "    try:\n",
        "        seed_js = get_uniprot_entry_json(seed_acc)\n",
        "    except Exception as e:\n",
        "        return None, {\"method\":\"uniprot-name\",\"status\":\"seed-fetch-failed\",\"detail\":str(e)}\n",
        "    gene, prefname = _seed_gene_and_names(seed_js)\n",
        "    if not gene:\n",
        "        return None, {\"method\":\"uniprot-name\",\"status\":\"no-gene\"}\n",
        "\n",
        "    q = f\"(gene:{gene}) AND (organism_id:{taxid})\"\n",
        "    if prefer_reviewed: q += \" AND (reviewed:true)\"\n",
        "    params = {\"query\": q, \"format\": \"json\", \"fields\": \"accession,reviewed,organism_id,protein_name\"}\n",
        "    try:\n",
        "        js = _get_json(f\"{UNI_KB}/search\", params=params)\n",
        "    except Exception as e:\n",
        "        return None, {\"method\":\"uniprot-name\",\"status\":\"search-failed\",\"detail\":str(e)}\n",
        "\n",
        "    hits = js.get(\"results\", [])\n",
        "    if not hits:\n",
        "        return None, {\"method\":\"uniprot-name\",\"status\":\"no-hit\"}\n",
        "\n",
        "    best = next((rec for rec in hits if rec.get(\"reviewed\") is True), None) or hits[0]\n",
        "    acc = best.get(\"primaryAccession\") or best.get(\"accession\")\n",
        "    return (acc if acc != seed_acc else None), {\"method\":\"uniprot-name\",\"status\":\"hit\",\"gene\":gene}\n",
        "\n",
        "# Verbose, defensive MMseqs2 wrapper used by both firstpass and deep-pass\n",
        "import os, subprocess, requests, gzip, io, time, re\n",
        "\n",
        "UNI_STREAM = \"https://rest.uniprot.org/uniprotkb/stream\"\n",
        "_HEADERS = {\"User-Agent\": \"OrthologFetcher/1.0\", \"Accept\": \"*/*\"}\n",
        "_ACC_RE = re.compile(r'^(?:[sptr]\\|)?([A-Z0-9]{6,10})')\n",
        "\n",
        "def _sh(cmd, check=True):\n",
        "    p = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
        "    if check and p.returncode != 0:\n",
        "        print(p.stdout)\n",
        "        raise subprocess.CalledProcessError(p.returncode, cmd)\n",
        "    return p.stdout\n",
        "\n",
        "def _ensure_query_fasta(acc: str) -> tuple[str, dict]:\n",
        "    os.makedirs(\"mmseqs_tmp\", exist_ok=True)\n",
        "    qf = f\"mmseqs_tmp/{acc}.fa\"\n",
        "    if os.path.exists(qf) and os.path.getsize(qf) > 0:\n",
        "        return qf, {\"method\":\"mmseqs2\",\"status\":\"query-cached\"}\n",
        "    params = {\"format\": \"fasta\", \"compressed\": \"true\", \"query\": f\"(accession:{acc})\"}\n",
        "    for attempt in range(4):\n",
        "        try:\n",
        "            r = requests.get(UNI_STREAM, params=params, headers=_HEADERS, timeout=60)\n",
        "            r.raise_for_status()\n",
        "            with gzip.GzipFile(fileobj=io.BytesIO(r.content)) as gz:\n",
        "                data = gz.read()\n",
        "            with open(qf, \"wb\") as out:\n",
        "                out.write(data)\n",
        "            # sanity: ensure header\n",
        "            with open(qf, \"rt\", errors=\"ignore\") as fh:\n",
        "                if not any(ln.startswith(\">\") for ln in fh):\n",
        "                    os.remove(qf)\n",
        "                    raise RuntimeError(\"FASTA has no header\")\n",
        "            return qf, {\"method\":\"mmseqs2\",\"status\":\"query-fetched\"}\n",
        "        except Exception as e:\n",
        "            if attempt == 3:\n",
        "                return \"\", {\"method\":\"mmseqs2\",\"status\":\"query-fetch-failed\",\"error\":str(e)}\n",
        "            time.sleep(2**attempt)\n",
        "    return \"\", {\"method\":\"mmseqs2\",\"status\":\"query-fetch-failed\",\"error\":\"unknown\"}\n",
        "\n",
        "def _extract_acc(target_field: str) -> str | None:\n",
        "    \"\"\"\n",
        "    Accepts 'sp|Q9W0V0|PROT_NAME ...' or 'Q9W0V0' and returns 'Q9W0V0'.\n",
        "    \"\"\"\n",
        "    if not target_field:\n",
        "        return None\n",
        "    t = target_field.split()[0]  # mmseqs uses ID up to first whitespace\n",
        "    m = _ACC_RE.match(t)\n",
        "    return m.group(1) if m else None\n",
        "\n",
        "def method_mmseqs2(seed_acc: str, taxid: int):\n",
        "    \"\"\"Return (best_target_acc | None, info_dict) with real metrics in info.\"\"\"\n",
        "    t0 = time.time()\n",
        "    dbdir = f\"dbs/{int(taxid)}\"\n",
        "    if not (os.path.exists(dbdir) or os.path.exists(dbdir + \".dbtype\")):\n",
        "        return None, {\"method\":\"mmseqs2\",\"status\":\"no-target-db\",\"db\":dbdir}\n",
        "\n",
        "    # Query FASTA\n",
        "    qf, qinfo = _ensure_query_fasta(seed_acc)\n",
        "    if not qf:\n",
        "        return None, qinfo\n",
        "\n",
        "    # Run mmseqs easy-search\n",
        "    outf = f\"mmseqs_tmp/{seed_acc}_{int(taxid)}.tsv\"\n",
        "    fmt = \"query,target,evalue,bits,qcov,tcov,alnlen,qlen,tlen,pident\"\n",
        "    cmd = [\n",
        "        \"mmseqs\",\"easy-search\", qf, dbdir, outf, \"mmseqs_tmp\",\n",
        "        \"-s\", str(MMSEQS2_SENS),\n",
        "        \"--max-seqs\", str(MMSEQS2_MAX_SEQS),\n",
        "        \"--format-output\", fmt,\n",
        "        \"--threads\", str(THREADS)\n",
        "    ]\n",
        "    try:\n",
        "        _sh(cmd, check=True)\n",
        "    except subprocess.CalledProcessError:\n",
        "        return None, {\"method\":\"mmseqs2\",\"status\":\"search-failed\",\"cmd\":\" \".join(cmd)}\n",
        "\n",
        "    # Parse, filter, keep best (min evalue, tie-break by max bits)\n",
        "    best = None\n",
        "    try:\n",
        "        with open(outf, \"rt\", errors=\"ignore\") as fh:\n",
        "            for ln in fh:\n",
        "                if not ln or ln.startswith(\"#\"):\n",
        "                    continue\n",
        "                parts = ln.rstrip(\"\\n\").split(\"\\t\")\n",
        "                if len(parts) < 10:\n",
        "                    continue\n",
        "                _, target_raw, ev_s, bits_s, qcov_s, tcov_s, alnlen_s, qlen_s, tlen_s, pident_s = parts[:10]\n",
        "\n",
        "                acc = _extract_acc(target_raw)\n",
        "                if not acc or acc == seed_acc:\n",
        "                    continue\n",
        "\n",
        "                # robust float parsing\n",
        "                try:\n",
        "                    ev     = float(ev_s)\n",
        "                except Exception:\n",
        "                    try:    ev = float(ev_s.replace(\"E\", \"e\"))\n",
        "                    except: ev = float(\"inf\")\n",
        "                try:    bits   = float(bits_s)\n",
        "                except: bits   = 0.0\n",
        "                try:    qcov   = float(qcov_s)\n",
        "                except: qcov   = 0.0\n",
        "                try:    tcov   = float(tcov_s)\n",
        "                except: tcov   = 0.0\n",
        "                try:    pident = float(pident_s)\n",
        "                except: pident = 0.0\n",
        "\n",
        "                # thresholds\n",
        "                if MMSEQS2_MAX_EVALUE is not None and ev > MMSEQS2_MAX_EVALUE: continue\n",
        "                if MMSEQS2_MIN_BITS   is not None and bits   < MMSEQS2_MIN_BITS:   continue\n",
        "                if MMSEQS2_MIN_PIDENT is not None and pident < MMSEQS2_MIN_PIDENT: continue\n",
        "                if MMSEQS2_MIN_QCOV   is not None and qcov   < MMSEQS2_MIN_QCOV:   continue\n",
        "                if MMSEQS2_MIN_TCOV   is not None and tcov   < MMSEQS2_MIN_TCOV:   continue\n",
        "\n",
        "                cand = {\n",
        "                    \"acc\": acc,\n",
        "                    \"evalue\": ev, \"bits\": bits,\n",
        "                    \"pident\": pident, \"qcov\": qcov, \"tcov\": tcov,\n",
        "                    \"alnlen\": alnlen_s, \"qlen\": qlen_s, \"tlen\": tlen_s\n",
        "                }\n",
        "\n",
        "                if best is None:\n",
        "                    best = cand\n",
        "                else:\n",
        "                    if (cand[\"evalue\"] < best[\"evalue\"]) or (\n",
        "                        cand[\"evalue\"] == best[\"evalue\"] and cand[\"bits\"] > best[\"bits\"]\n",
        "                    ):\n",
        "                        best = cand\n",
        "    except FileNotFoundError:\n",
        "        return None, {\"method\":\"mmseqs2\",\"status\":\"no-tsv\"}\n",
        "\n",
        "    dur = round(time.time()-t0, 3)\n",
        "    if best:\n",
        "        info = {\"method\":\"mmseqs2\",\"status\":\"hit\",\"time_s\":dur}\n",
        "        info.update(best)  # <- attach metrics so your stdout can show them\n",
        "        return best[\"acc\"], info\n",
        "\n",
        "    return None, {\"method\":\"mmseqs2\",\"status\":\"no-hit-after-filter\",\"time_s\":dur}\n",
        "\n",
        "def _afdb_model_to_file(acc: str, dest_pdb: str) -> bool:\n",
        "    for ver in (\"v4\",\"v3\"):\n",
        "        url = f\"https://alphafold.ebi.ac.uk/files/AF-{acc}-F1-model_{ver}.pdb\"\n",
        "        r = requests.get(url, timeout=45)\n",
        "        if r.status_code == 200 and \"ATOM\" in r.text:\n",
        "            with open(dest_pdb, \"w\") as f: f.write(r.text)\n",
        "            return True\n",
        "    for ver in (\"v4\",\"v3\"):\n",
        "        url = f\"https://alphafold.ebi.ac.uk/files/AF-{acc}-F1-model_{ver}.cif\"\n",
        "        r = requests.get(url, timeout=45)\n",
        "        if r.status_code == 200 and \"_atom_site\" in r.text:\n",
        "            with open(dest_pdb, \"w\") as f: f.write(r.text)\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "def method_foldseek(seed_acc, taxid):\n",
        "    if not USE_FOLDSEEK or not FOLDSEEK_DB or not os.path.exists(FOLDSEEK_DB):\n",
        "        return None, {\"method\":\"foldseek\",\"status\":\"disabled-or-missing-db\"}\n",
        "\n",
        "    qtmp = tempfile.NamedTemporaryFile(delete=False, suffix=\".pdb\"); qtmp.close()\n",
        "    if not _afdb_model_to_file(seed_acc, qtmp.name):\n",
        "        try: os.remove(qtmp.name)\n",
        "        except: pass\n",
        "        return None, {\"method\":\"foldseek\",\"status\":\"no-structure\"}\n",
        "\n",
        "    out_tsv = f\"mmseqs_tmp/{seed_acc}_{taxid}.fold.tsv\"\n",
        "    fmt = \"query,target,evalue,bits,qcov,tcov,alntmscore,qtmscore,ttmscore,lddt\"\n",
        "    cmd = [\n",
        "        \"foldseek\",\"easy-search\",\n",
        "        qtmp.name, FOLDSEEK_DB, out_tsv, \"mmseqs_tmp\",\n",
        "        \"-s\", str(FOLDSEEK_SENS),\n",
        "        \"--max-seqs\", str(FOLDSEEK_MAX_SEQS),\n",
        "        \"--alignment-type\", str(FOLDSEEK_ALIGNMENT_TYPE),\n",
        "        \"--format-output\", fmt,\n",
        "        \"-e\", str(FOLDSEEK_MAX_EVALUE),\n",
        "        \"-c\", str(FOLDSEEK_MIN_QCOV),\n",
        "        \"--cov-mode\", str(FOLDSEEK_COV_MODE)\n",
        "    ]\n",
        "    proc = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
        "    if proc.returncode != 0:\n",
        "        try: os.remove(qtmp.name)\n",
        "        except: pass\n",
        "        return None, {\"method\":\"foldseek\",\"status\":\"error\",\"detail\":proc.stdout.splitlines()[-1:]}\n",
        "\n",
        "    best=None\n",
        "    try:\n",
        "        with open(out_tsv, newline=\"\") as fh:\n",
        "            rdr = csv.reader(fh, delimiter=\"\\t\")\n",
        "            for row in rdr:\n",
        "                if len(row) < 10: continue\n",
        "                _, target, evalue, bits, qcov, tcov, alntms, qtms, ttms, lddt = row\n",
        "                m = re.search(r\"(?:\\b[sptr]\\|)?([A-Z0-9]{6,10})\", target)\n",
        "                acc = m.group(1) if m else None\n",
        "                if not acc or acc == seed_acc: continue\n",
        "\n",
        "                ev   = float(evalue) if evalue not in (\"\",\"NA\") else math.inf\n",
        "                qc   = float(qcov)   if qcov   not in (\"\",\"NA\") else 0.0\n",
        "                tc   = float(tcov)   if tcov   not in (\"\",\"NA\") else 0.0\n",
        "                atms = float(alntms) if alntms not in (\"\",\"NA\") else 0.0\n",
        "                ldt  = float(lddt)   if lddt   not in (\"\",\"NA\") else 0.0\n",
        "\n",
        "                if FOLDSEEK_MAX_EVALUE   is not None and ev > FOLDSEEK_MAX_EVALUE:   continue\n",
        "                if FOLDSEEK_MIN_ALNTMS   is not None and atms < FOLDSEEK_MIN_ALNTMS: continue\n",
        "                if FOLDSEEK_MIN_LDDT     is not None and ldt  < FOLDSEEK_MIN_LDDT:   continue\n",
        "                if FOLDSEEK_MIN_QCOV     is not None and qc   < FOLDSEEK_MIN_QCOV:   continue\n",
        "                if FOLDSEEK_MIN_TCOV     is not None and tc   < FOLDSEEK_MIN_TCOV:   continue\n",
        "\n",
        "                cand = {\"acc\":acc,\"evalue\":evalue,\"bits\":bits,\n",
        "                        \"alntmscore\":alntms,\"qtmscore\":qtms,\"ttmscore\":ttms,\n",
        "                        \"lddt\":lddt,\"qcov\":qcov,\"tcov\":tcov}\n",
        "                if best is None or float(cand[\"alntmscore\"]) > float(best[\"alntmscore\"]):\n",
        "                    best = cand\n",
        "        if best:\n",
        "            return best[\"acc\"], {\"method\":\"foldseek\",\"status\":\"hit\", **best}\n",
        "        return None, {\"method\":\"foldseek\",\"status\":\"no-hit-after-filter\"}\n",
        "    finally:\n",
        "        try: os.remove(qtmp.name)\n",
        "        except: pass\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "mpcQqV8d2jOf"
      },
      "id": "mpcQqV8d2jOf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title üö¶ Orchestrators\n",
        "def find_ortholog_firstpass(seed_acc, taxid, species_name):\n",
        "    tried = []\n",
        "\n",
        "    acc, info = method_uniprot_quick_name(seed_acc, taxid, prefer_reviewed=True)\n",
        "    tried.append(info.copy())\n",
        "    if acc:\n",
        "        if VERBOSE: print(f\"[OK] {species_name}({taxid}) {seed_acc} ‚Üê UniProt {acc}\")\n",
        "        return acc, info, tried\n",
        "\n",
        "    acc, info = method_mmseqs2(seed_acc, taxid)\n",
        "    tried.append(info.copy())\n",
        "    if acc:\n",
        "        if VERBOSE:\n",
        "            print(f\"[OK] {species_name}({taxid}) {seed_acc} ‚Üê MMseqs2 {acc} \"\n",
        "                  f\"e={info.get('evalue')} bits={info.get('bits')} \"\n",
        "                  f\"pident={info.get('pident')} qcov={info.get('qcov')} tcov={info.get('tcov')}\")\n",
        "        return acc, info, tried\n",
        "\n",
        "    acc, info = method_foldseek(seed_acc, taxid)\n",
        "    tried.append(info.copy())\n",
        "    if acc:\n",
        "        if VERBOSE:\n",
        "            print(f\"[OK] {species_name}({taxid}) {seed_acc} ‚Üê Foldseek {acc} \"\n",
        "                  f\"alnTM={info.get('alntmscore')} lddt={info.get('lddt')}\")\n",
        "        return acc, info, tried\n",
        "\n",
        "    if VERBOSE: print(f\"[MISS] {species_name}({taxid}) {seed_acc}\")\n",
        "    return None, {\"method\":\"none\",\"status\":\"miss\"}, tried\n",
        "\n",
        "\n",
        "# Deep pass: omit UniProt text; try MMseqs2 ‚Üí Foldseek (if enabled)\n",
        "def find_ortholog_deeppass(seed_acc: str, taxid: int, species_name: str):\n",
        "    tried = []\n",
        "    if USE_MMSEQS2:\n",
        "        acc, info = method_mmseqs2(seed_acc, taxid)  # 2-arg\n",
        "        tried.append(info)\n",
        "        if acc:\n",
        "            return acc, info, tried\n",
        "    if USE_FOLDSEEK:\n",
        "        try:\n",
        "            acc, info = method_foldseek(seed_acc, taxid)\n",
        "        except TypeError:\n",
        "            acc, info = method_foldseek(seed_acc, taxid, species_name)\n",
        "        tried.append(info)\n",
        "        if acc:\n",
        "            return acc, info, tried\n",
        "    return None, {\"method\":\"deeppass\",\"status\":\"miss\"}, tried"
      ],
      "metadata": {
        "cellView": "form",
        "id": "IZSRHarj2m2A"
      },
      "id": "IZSRHarj2m2A",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ‚ñ∂Ô∏è Run (first pass) ‚Äî with seed-origin shortcut + result cache\n",
        "from collections import defaultdict\n",
        "\n",
        "seed_accs = SEED_LIST\n",
        "chain_names = CHAIN_ALIASES\n",
        "assert len(seed_accs) == len(chain_names)\n",
        "\n",
        "# Ensure we have seed origins (seed accession -> taxid)\n",
        "try:\n",
        "    SEED_ORIGINS\n",
        "except NameError:\n",
        "    # Fallback: compute if not present in this runtime (uses your helper)\n",
        "    SEED_ORIGINS = {acc: up_taxid_of_acc(acc) for acc in seed_accs}\n",
        "\n",
        "methods_log = []\n",
        "rows = []\n",
        "\n",
        "# Cache results of firstpass to avoid re-running identical (seed, taxid)\n",
        "_firstpass_cache = {}  # (seed, taxid) -> (acc, info, tried)\n",
        "\n",
        "for sp, taxid in ALL_SPECIES:\n",
        "    if not taxid:\n",
        "        print(f\"[SKIP] No taxid for {sp}\")\n",
        "        continue\n",
        "\n",
        "    row = {\"species_name\": sp, \"taxid\": taxid}\n",
        "\n",
        "    for seed, chain in zip(seed_accs, chain_names):\n",
        "        # 1) Seed-origin shortcut\n",
        "        origin_tid = SEED_ORIGINS.get(seed)\n",
        "        if origin_tid and int(origin_tid) == int(taxid):\n",
        "            row[chain] = seed\n",
        "            methods_log.append({\n",
        "                \"species_name\": sp, \"taxid\": taxid, \"seed\": seed, \"chain\": chain,\n",
        "                \"found_acc\": seed, \"method\": \"seed-origin\", \"status\": \"origin\"\n",
        "            })\n",
        "            continue\n",
        "\n",
        "        # 2) Cached firstpass result?\n",
        "        key = (seed, int(taxid))\n",
        "        if key in _firstpass_cache:\n",
        "            acc, info, tried = _firstpass_cache[key]\n",
        "        else:\n",
        "            acc, info, tried = find_ortholog_firstpass(seed, taxid, sp)\n",
        "            _firstpass_cache[key] = (acc, info, tried)\n",
        "\n",
        "        # 3) Write cell + logging\n",
        "        row[chain] = acc or \"\"\n",
        "        methods_log.append({\n",
        "            \"species_name\": sp, \"taxid\": taxid, \"seed\": seed, \"chain\": chain,\n",
        "            \"found_acc\": acc or \"\", \"method\": info.get(\"method\",\"none\"),\n",
        "            \"status\": info.get(\"status\",\"miss\"),\n",
        "            **{k:v for k,v in info.items() if k not in (\"method\",\"status\")}\n",
        "        })\n",
        "        for step in tried:\n",
        "            methods_log.append({\n",
        "                \"species_name\": sp, \"taxid\": taxid, \"seed\": seed, \"chain\": chain,\n",
        "                \"found_acc\": acc or \"\", \"method\": step.get(\"method\",\"\"),\n",
        "                \"status\": step.get(\"status\",\"\"),\n",
        "                **{k:v for k,v in step.items() if k not in (\"method\",\"status\")}\n",
        "            })\n",
        "\n",
        "    rows.append(row)\n",
        "\n",
        "# Build and save outputs\n",
        "df = pd.DataFrame(rows, columns=[\"species_name\",\"taxid\"] + chain_names)\n",
        "ts = pd.Timestamp.now().strftime('%Y%m%d-%H%M%S')\n",
        "out_csv = f\"{OUTPUT_BASENAME}_{ts}.csv\"\n",
        "out_methods = f\"{OUTPUT_BASENAME}_{ts}_methods.csv\"\n",
        "df.to_csv(out_csv, index=False)\n",
        "pd.DataFrame(methods_log).to_csv(out_methods, index=False)\n",
        "\n",
        "print(\"\\nSaved:\")\n",
        "print(\" - Results:\", out_csv)\n",
        "print(\" - Methods:\", out_methods)\n",
        "df\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "ZXdf2dlw2o6e"
      },
      "id": "ZXdf2dlw2o6e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title üîÅ Deep Search (Repeatable)\n",
        "DEEP_SEARCH_ROUNDS = 1     #@param {type:\"integer\"}\n",
        "DEEP_MAX_COL_CAND = 32     #@param {type:\"integer\"}\n",
        "DEEP_VERBOSE = True        #@param {type:\"boolean\"}\n",
        "DEEP_RESET_CACHE = True   #@param {type:\"boolean\"}\n",
        "\n",
        "# Persist cache across runs unless reset\n",
        "if DEEP_RESET_CACHE or (\"_deep_cache\" not in globals()):\n",
        "    _deep_cache = {}  # (query_acc, taxid) -> (hit_acc | None, info_dict)\n",
        "\n",
        "def _nonempty_vals(series):\n",
        "    out, seen = [], set()\n",
        "    for v in series.tolist():\n",
        "        if isinstance(v, str):\n",
        "            vv = v.strip()\n",
        "            if vv and vv not in seen:\n",
        "                seen.add(vv); out.append(vv)\n",
        "    return out\n",
        "\n",
        "def _gather_same_chain_candidates(df, chain_col, seed_acc_for_chain):\n",
        "    col_cands = _nonempty_vals(df[chain_col])\n",
        "    if DEEP_MAX_COL_CAND > 0:\n",
        "        col_cands = col_cands[:DEEP_MAX_COL_CAND]\n",
        "    if seed_acc_for_chain and seed_acc_for_chain not in set(col_cands):\n",
        "        col_cands.append(seed_acc_for_chain)\n",
        "    return col_cands\n",
        "\n",
        "# -------- pretty-print helpers --------\n",
        "def _fmt_metrics(info: dict) -> str:\n",
        "    def f(k, fmt):\n",
        "        v = info.get(k)\n",
        "        if v is None or v == \"\":\n",
        "            return None\n",
        "        try:\n",
        "            return fmt(float(v))\n",
        "        except Exception:\n",
        "            return None\n",
        "    parts = []\n",
        "    e  = f(\"evalue\", lambda x: f\"e={x:.3e}\")\n",
        "    b  = f(\"bits\",   lambda x: f\"bits={x:.1f}\")\n",
        "    pi = f(\"pident\", lambda x: f\"pident={x:.1f}\")\n",
        "    qc = f(\"qcov\",   lambda x: f\"qcov={x:.3f}\")\n",
        "    tc = f(\"tcov\",   lambda x: f\"tcov={x:.3f}\")\n",
        "    for p in (e,b,pi,qc,tc):\n",
        "        if p: parts.append(p)\n",
        "    return \" \".join(parts)\n",
        "\n",
        "def _label_method(m):\n",
        "    m = (m or \"\").lower()\n",
        "    if m == \"mmseqs2\": return \"MMseqs2\"\n",
        "    if m == \"foldseek\": return \"Foldseek\"\n",
        "    return m or \"MMseqs2\"\n",
        "\n",
        "# ----------------------------------------------------\n",
        "\n",
        "def _deep_try_fill_cell(df, r, chain_col, species_name, taxid, seed_acc_for_chain):\n",
        "    # already filled?\n",
        "    cur = df.at[r, chain_col]\n",
        "    if isinstance(cur, str) and cur.strip():\n",
        "        return False\n",
        "\n",
        "    cands = _gather_same_chain_candidates(df, chain_col, seed_acc_for_chain)\n",
        "    if DEEP_VERBOSE:\n",
        "        print(f\"[DEEP] {species_name}({taxid}) {chain_col} will try {len(cands)} candidate(s)\")\n",
        "\n",
        "    for q in cands:\n",
        "        key = (q, int(taxid))\n",
        "        if key in _deep_cache:\n",
        "            acc, info = _deep_cache[key]\n",
        "        else:\n",
        "            acc, info, _ = find_ortholog_deeppass(q, int(taxid), species_name)\n",
        "            _deep_cache[key] = (acc, info)\n",
        "\n",
        "        metrics = _fmt_metrics(info)\n",
        "        method_label = _label_method(info.get(\"method\"))\n",
        "\n",
        "        if acc:\n",
        "            df.at[r, chain_col] = acc\n",
        "            print(f\"[OK] {species_name}({taxid}) {seed_acc_for_chain} via {q} ‚Üê {method_label} {acc}\"\n",
        "                  + (f\" {metrics}\" if metrics else \"\"))\n",
        "            log_row = {\n",
        "                \"species_name\": species_name, \"taxid\": int(taxid),\n",
        "                \"seed\": seed_acc_for_chain, \"chain\": chain_col,\n",
        "                \"found_acc\": acc, \"method\": info.get(\"method\",\"\"),\n",
        "                \"status\": info.get(\"status\",\"\"), \"deep_round\": True,\n",
        "                \"via\": q\n",
        "            }\n",
        "            for k in (\"evalue\",\"bits\",\"pident\",\"qcov\",\"tcov\",\"alnlen\",\"qlen\",\"tlen\",\"time_s\"):\n",
        "                if k in info:\n",
        "                    log_row[k] = info[k]\n",
        "            methods_log.append(log_row)\n",
        "            return True\n",
        "        else:\n",
        "            print(f\"[MISS] {species_name}({taxid}) {seed_acc_for_chain} via {q}\")\n",
        "\n",
        "    return False\n",
        "\n",
        "# ---- Run deep rounds ----\n",
        "fills_total = 0\n",
        "for rnd in range(1, int(DEEP_SEARCH_ROUNDS) + 1):\n",
        "    fills_this = 0\n",
        "    if DEEP_VERBOSE:\n",
        "        print(f\"\\n=== Deep round {rnd} ===\")\n",
        "    for r, (sp, tid) in enumerate(zip(df[\"species_name\"], df[\"taxid\"])):\n",
        "        if not tid:\n",
        "            continue\n",
        "        for seed, chain in zip(SEED_LIST, CHAIN_ALIASES):\n",
        "            if _deep_try_fill_cell(df, r, chain, sp, tid, seed):\n",
        "                fills_this += 1\n",
        "\n",
        "    fills_total += fills_this\n",
        "    if DEEP_VERBOSE:\n",
        "        print(f\"[Deep] round {rnd} filled {fills_this}\")\n",
        "    if fills_this == 0:\n",
        "        if DEEP_VERBOSE:\n",
        "            print(\"[Deep] No more fills.\")\n",
        "        break\n",
        "\n",
        "# ---- Save merged results ----\n",
        "ts = pd.Timestamp.now().strftime('%Y%m%d-%H%M%S')\n",
        "out_csv = f\"{OUTPUT_BASENAME}_{ts}.csv\"\n",
        "out_methods = f\"{OUTPUT_BASENAME}_{ts}_methods.csv\"\n",
        "df.to_csv(out_csv, index=False)\n",
        "pd.DataFrame(methods_log).to_csv(out_methods, index=False)\n",
        "print(\"\\nSaved (first + deep):\")\n",
        "print(\" - Results:\", out_csv)\n",
        "print(\" - Methods:\", out_methods)\n",
        "df\n"
      ],
      "metadata": {
        "id": "YB19ZyZY2qrL",
        "cellView": "form"
      },
      "id": "YB19ZyZY2qrL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title üì• Download results\n",
        "from google.colab import files, runtime\n",
        "import os\n",
        "if \"out_csv\" in globals() and os.path.exists(out_csv):\n",
        "    files.download(out_csv)\n",
        "if \"out_methods\" in globals() and os.path.exists(out_methods):\n",
        "    files.download(out_methods)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "w6FYvvT22tvj"
      },
      "id": "w6FYvvT22tvj",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}